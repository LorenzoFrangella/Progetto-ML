{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/site-packages (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/site-packages (from gymnasium) (1.24.3)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/site-packages (from gymnasium) (0.0.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /usr/local/lib/python3.10/site-packages (from gymnasium) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/site-packages (from gymnasium) (4.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/usr/local/opt/python@3.10/bin/python3.10 -m pip install --upgrade pip\u001b[0m\n",
      "zsh:1: no matches found: gym[mujoco]\n"
     ]
    }
   ],
   "source": [
    "# Code to run in only in colab for packet download\n",
    "!pip3 install gymnasium\n",
    "!pip3 install gym[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-27 19:21:28.597630: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math \n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/glfw/__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# test of the environment\n",
    "\n",
    "env = gym.make('Walker2d-v4',render_mode=\"human\") #change w \"human\" if needed\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close\n",
    "\n",
    "\n",
    "OBSERVATIONS = env.observation_space.shape[0]\n",
    "ACTIONS = 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_continue_action(index):\n",
    "    value = (2/ACTIONS) * index - 1\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 128)\n",
    "        self.layer5 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "print(OBSERVATIONS)\n",
    "\n",
    "main_nn = []\n",
    "target_nn = []\n",
    "optimizer_arr = []\n",
    "\n",
    "for i in range(6):\n",
    "    main_nn.append(DQN(OBSERVATIONS,ACTIONS).to(device))\n",
    "    target_nn.append(DQN(OBSERVATIONS,ACTIONS).to(device))\n",
    "    optimizer_arr.append(torch.optim.Adam(main_nn[i].parameters(), lr=LEARNING_RATE))\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, size, device =\"cpu\"):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self,state,action, reward, next_state, done):\n",
    "        self.buffer.append((state, action,reward,next_state,done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [] , [] , [] , [] , []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        qs = []\n",
    "        action_decimal = []\n",
    "        for i in range(6):\n",
    "            qs.append(main_nn[i](state).cpu().data.numpy())\n",
    "            action_decimal.append((np.argmax(qs[i])*(2/ACTIONS))-1)\n",
    "        return action_decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(states, actions, rewards, next_states, dones):\n",
    "    max_next_qs = []\n",
    "    target = []\n",
    "    qs = []\n",
    "    action_masks = []\n",
    "    masked_qs = []\n",
    "    loss = []\n",
    "    for i in range(6):\n",
    "        max_next_qs.append(target_nn[i](next_states).max(-1).values)\n",
    "        target.append(rewards + (1.0 - dones) * DISCOUNT * max_next_qs[i])\n",
    "        qs.append(main_nn[i](states))\n",
    "        actions_transposed = torch.transpose(actions,0,1)\n",
    "        action_masks.append(F.one_hot(actions_transposed[i].type(torch.int64), ACTIONS))\n",
    "        masked_qs.append((action_masks[i] * qs[i]).sum(dim=-1))\n",
    "        loss.append(loss_fn(masked_qs[i], target[i].detach()))\n",
    "        optimizer_arr[i].zero_grad()\n",
    "        loss[i].backward()\n",
    "        optimizer_arr[i].step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000. Epsilon: -0.001. Reward in last 100 episodes: 1.99, loss: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb Cella 10\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     dones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(dones)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(actions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     loss \u001b[39m=\u001b[39m training_step(states, actions, rewards, next_states, dones)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m<\u001b[39m \u001b[39m950\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m   epsilon \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n",
      "\u001b[1;32m/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb Cella 10\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss\u001b[39m.\u001b[39mappend(loss_fn(masked_qs[i], target[i]\u001b[39m.\u001b[39mdetach()))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     optimizer_arr[i]\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     loss[i]\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optimizer_arr[i]\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Walker2Dimplementation.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 10000\n",
    "epsilon = 0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayMemory(100000, device=device)\n",
    "cur_frame = 0\n",
    "loss = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "  state = env.reset()[0].astype(np.float32)\n",
    "  ep_reward, done = 0, False\n",
    "  while not done:\n",
    "    state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "    next_state, reward, done, info, unknown_attribute = env.step(action)\n",
    "    next_state = next_state.astype(np.float32)\n",
    "    ep_reward += reward\n",
    "    # Save to experience replay.\n",
    "    integer_action = []\n",
    "    for i in range(6):\n",
    "      integer_action.append((action[i] + 1)*(ACTIONS/2))\n",
    "    buffer.add(state, integer_action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    cur_frame += 1\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 2000 == 0:\n",
    "      for i in range(6):\n",
    "        target_nn[i].load_state_dict(main_nn[i].state_dict())\n",
    "    \n",
    "    # Train neural network.\n",
    "    if len(buffer) > batch_size:\n",
    "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "      next_states = torch.from_numpy(next_states)\n",
    "      rewards = torch.from_numpy(rewards)\n",
    "      states = torch.from_numpy(states)\n",
    "      dones = torch.from_numpy(dones)\n",
    "      actions = torch.from_numpy(actions)\n",
    "\n",
    "\n",
    "      loss = training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "  if episode < 950:\n",
    "    epsilon -= 0.001\n",
    "\n",
    "  if len(last_100_ep_rewards) == 100:\n",
    "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "  last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "  if episode % 50 == 0:\n",
    "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}, loss: {loss}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"./2D/\"\n",
    "for i in range(6):\n",
    "    main_nn[i].load_state_dict(torch.load(PATH + f\"RNN{i}.param\",map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNon è possibile eseguire il codice. La sessione è stata eliminata. Provare a riavviare il kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "env_test = gym.make('Walker2d-v4',render_mode=\"human\")\n",
    "\n",
    "observation = env_test.reset()[0].astype(np.float32)\n",
    "\n",
    "\n",
    "for i in range(100000):\n",
    "    inp = torch.from_numpy(observation)\n",
    "    action = select_epsilon_greedy_action(inp,0)\n",
    "    observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "    observation = observation.astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "\n",
    "\n",
    "env_test.close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
