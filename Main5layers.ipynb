{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Machine Learning\n",
    "#### Double inverted pendulum - Lorenzo Frangella 1899674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run in only in colab for packet download\n",
    "!pip3 install gymnasium\n",
    "!pip3 install gym[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-24 14:05:36.997762: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math \n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters used in our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 1e-4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test of the environment\n",
    "\n",
    "env = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\") #change w \"human\" if needed\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close\n",
    "\n",
    "\n",
    "OBSERVATIONS = env.observation_space.shape[0]\n",
    "ACTIONS = 64\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the output of the neural network to the interval -1,1\n",
    "Since as action we have only one scalar value that represent magnitude and direction of the force applied to the cart, we have to \"cast\" the output of the neural network into a scalar contiguos value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_continue_action(index):\n",
    "    value = (2/ACTIONS) * index - 1\n",
    "    return value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the neural network needed for DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 128)\n",
    "        self.layer5 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "print(OBSERVATIONS)\n",
    "\n",
    "main_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "target_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data structure that is needed for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, size, device =\"cpu\"):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self,state,action, reward, next_state, done):\n",
    "        self.buffer.append((state, action,reward,next_state,done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [] , [] , [] , [] , []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part of code is needed to develop a epsilon-greedy policy\n",
    "\n",
    "With a given probability epsilon a random action is choosen otherwise is choosen the best action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        action_decimal = np.array([(np.argmax(qs)*(2/ACTIONS))-1])\n",
    "        return action_decimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a train step that is performed on the replay memory of size batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(states, actions, rewards, next_states, dones):\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * DISCOUNT * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions.type(torch.int64), ACTIONS)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000. Epsilon: 0.999. Reward in last 100 episodes: 36.04\n",
      "Episode 50/10000. Epsilon: 0.949. Reward in last 100 episodes: 52.63\n",
      "Episode 100/10000. Epsilon: 0.899. Reward in last 100 episodes: 57.01\n",
      "Episode 150/10000. Epsilon: 0.849. Reward in last 100 episodes: 59.67\n",
      "Episode 200/10000. Epsilon: 0.799. Reward in last 100 episodes: 56.24\n",
      "Episode 250/10000. Epsilon: 0.749. Reward in last 100 episodes: 53.20\n",
      "Episode 300/10000. Epsilon: 0.699. Reward in last 100 episodes: 53.18\n",
      "Episode 350/10000. Epsilon: 0.649. Reward in last 100 episodes: 53.96\n",
      "Episode 400/10000. Epsilon: 0.599. Reward in last 100 episodes: 53.67\n",
      "Episode 450/10000. Epsilon: 0.549. Reward in last 100 episodes: 53.31\n",
      "Episode 500/10000. Epsilon: 0.499. Reward in last 100 episodes: 53.98\n",
      "Episode 550/10000. Epsilon: 0.449. Reward in last 100 episodes: 52.28\n",
      "Episode 600/10000. Epsilon: 0.399. Reward in last 100 episodes: 53.84\n",
      "Episode 650/10000. Epsilon: 0.349. Reward in last 100 episodes: 56.29\n",
      "Episode 700/10000. Epsilon: 0.299. Reward in last 100 episodes: 54.99\n",
      "Episode 750/10000. Epsilon: 0.249. Reward in last 100 episodes: 55.36\n",
      "Episode 800/10000. Epsilon: 0.199. Reward in last 100 episodes: 54.73\n",
      "Episode 850/10000. Epsilon: 0.149. Reward in last 100 episodes: 52.63\n",
      "Episode 900/10000. Epsilon: 0.099. Reward in last 100 episodes: 52.23\n",
      "Episode 950/10000. Epsilon: 0.050. Reward in last 100 episodes: 53.83\n",
      "Episode 1000/10000. Epsilon: 0.050. Reward in last 100 episodes: 54.24\n",
      "Episode 1050/10000. Epsilon: 0.050. Reward in last 100 episodes: 53.10\n",
      "Episode 1100/10000. Epsilon: 0.050. Reward in last 100 episodes: 52.06\n",
      "Episode 1150/10000. Epsilon: 0.050. Reward in last 100 episodes: 53.58\n",
      "Episode 1200/10000. Epsilon: 0.050. Reward in last 100 episodes: 58.33\n",
      "Episode 1250/10000. Epsilon: 0.050. Reward in last 100 episodes: 59.27\n",
      "Episode 1300/10000. Epsilon: 0.050. Reward in last 100 episodes: 55.70\n",
      "Episode 1350/10000. Epsilon: 0.050. Reward in last 100 episodes: 51.96\n",
      "Episode 1400/10000. Epsilon: 0.050. Reward in last 100 episodes: 53.01\n",
      "Episode 1450/10000. Epsilon: 0.050. Reward in last 100 episodes: 54.05\n",
      "Episode 1500/10000. Epsilon: 0.050. Reward in last 100 episodes: 53.94\n",
      "Episode 1550/10000. Epsilon: 0.050. Reward in last 100 episodes: 59.94\n",
      "Episode 1600/10000. Epsilon: 0.050. Reward in last 100 episodes: 61.44\n",
      "Episode 1650/10000. Epsilon: 0.050. Reward in last 100 episodes: 65.30\n",
      "Episode 1700/10000. Epsilon: 0.050. Reward in last 100 episodes: 60.57\n",
      "Episode 1750/10000. Epsilon: 0.050. Reward in last 100 episodes: 54.65\n",
      "Episode 1800/10000. Epsilon: 0.050. Reward in last 100 episodes: 60.69\n",
      "Episode 1850/10000. Epsilon: 0.050. Reward in last 100 episodes: 64.67\n",
      "Episode 1900/10000. Epsilon: 0.050. Reward in last 100 episodes: 68.12\n",
      "Episode 1950/10000. Epsilon: 0.050. Reward in last 100 episodes: 65.57\n",
      "Episode 2000/10000. Epsilon: 0.050. Reward in last 100 episodes: 62.38\n",
      "Episode 2050/10000. Epsilon: 0.050. Reward in last 100 episodes: 63.21\n",
      "Episode 2100/10000. Epsilon: 0.050. Reward in last 100 episodes: 64.00\n",
      "Episode 2150/10000. Epsilon: 0.050. Reward in last 100 episodes: 64.73\n",
      "Episode 2200/10000. Epsilon: 0.050. Reward in last 100 episodes: 66.59\n",
      "Episode 2250/10000. Epsilon: 0.050. Reward in last 100 episodes: 64.81\n",
      "Episode 2300/10000. Epsilon: 0.050. Reward in last 100 episodes: 67.80\n",
      "Episode 2350/10000. Epsilon: 0.050. Reward in last 100 episodes: 75.11\n",
      "Episode 2400/10000. Epsilon: 0.050. Reward in last 100 episodes: 73.44\n",
      "Episode 2450/10000. Epsilon: 0.050. Reward in last 100 episodes: 72.90\n",
      "Episode 2500/10000. Epsilon: 0.050. Reward in last 100 episodes: 73.43\n",
      "Episode 2550/10000. Epsilon: 0.050. Reward in last 100 episodes: 74.93\n",
      "Episode 2600/10000. Epsilon: 0.050. Reward in last 100 episodes: 69.90\n",
      "Episode 2650/10000. Epsilon: 0.050. Reward in last 100 episodes: 62.35\n",
      "Episode 2700/10000. Epsilon: 0.050. Reward in last 100 episodes: 67.59\n",
      "Episode 2750/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.23\n",
      "Episode 2800/10000. Epsilon: 0.050. Reward in last 100 episodes: 77.73\n",
      "Episode 2850/10000. Epsilon: 0.050. Reward in last 100 episodes: 74.59\n",
      "Episode 2900/10000. Epsilon: 0.050. Reward in last 100 episodes: 74.89\n",
      "Episode 2950/10000. Epsilon: 0.050. Reward in last 100 episodes: 74.16\n",
      "Episode 3000/10000. Epsilon: 0.050. Reward in last 100 episodes: 72.90\n",
      "Episode 3050/10000. Epsilon: 0.050. Reward in last 100 episodes: 71.10\n",
      "Episode 3100/10000. Epsilon: 0.050. Reward in last 100 episodes: 72.02\n",
      "Episode 3150/10000. Epsilon: 0.050. Reward in last 100 episodes: 75.81\n",
      "Episode 3200/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.40\n",
      "Episode 3250/10000. Epsilon: 0.050. Reward in last 100 episodes: 73.82\n",
      "Episode 3300/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.67\n",
      "Episode 3350/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.97\n",
      "Episode 3400/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.24\n",
      "Episode 3450/10000. Epsilon: 0.050. Reward in last 100 episodes: 79.46\n",
      "Episode 3500/10000. Epsilon: 0.050. Reward in last 100 episodes: 77.04\n",
      "Episode 3550/10000. Epsilon: 0.050. Reward in last 100 episodes: 73.82\n",
      "Episode 3600/10000. Epsilon: 0.050. Reward in last 100 episodes: 73.51\n",
      "Episode 3650/10000. Epsilon: 0.050. Reward in last 100 episodes: 74.75\n",
      "Episode 3700/10000. Epsilon: 0.050. Reward in last 100 episodes: 77.78\n",
      "Episode 3750/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.55\n",
      "Episode 3800/10000. Epsilon: 0.050. Reward in last 100 episodes: 74.71\n",
      "Episode 3850/10000. Epsilon: 0.050. Reward in last 100 episodes: 78.64\n",
      "Episode 3900/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.58\n",
      "Episode 3950/10000. Epsilon: 0.050. Reward in last 100 episodes: 73.37\n",
      "Episode 4000/10000. Epsilon: 0.050. Reward in last 100 episodes: 71.75\n",
      "Episode 4050/10000. Epsilon: 0.050. Reward in last 100 episodes: 79.62\n",
      "Episode 4100/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.12\n",
      "Episode 4150/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.88\n",
      "Episode 4200/10000. Epsilon: 0.050. Reward in last 100 episodes: 77.62\n",
      "Episode 4250/10000. Epsilon: 0.050. Reward in last 100 episodes: 70.52\n",
      "Episode 4300/10000. Epsilon: 0.050. Reward in last 100 episodes: 75.61\n",
      "Episode 4350/10000. Epsilon: 0.050. Reward in last 100 episodes: 80.58\n",
      "Episode 4400/10000. Epsilon: 0.050. Reward in last 100 episodes: 78.85\n",
      "Episode 4450/10000. Epsilon: 0.050. Reward in last 100 episodes: 83.96\n",
      "Episode 4500/10000. Epsilon: 0.050. Reward in last 100 episodes: 91.44\n",
      "Episode 4550/10000. Epsilon: 0.050. Reward in last 100 episodes: 86.97\n",
      "Episode 4600/10000. Epsilon: 0.050. Reward in last 100 episodes: 83.03\n",
      "Episode 4650/10000. Epsilon: 0.050. Reward in last 100 episodes: 79.64\n",
      "Episode 4700/10000. Epsilon: 0.050. Reward in last 100 episodes: 80.08\n",
      "Episode 4750/10000. Epsilon: 0.050. Reward in last 100 episodes: 91.43\n",
      "Episode 4800/10000. Epsilon: 0.050. Reward in last 100 episodes: 84.37\n",
      "Episode 4850/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.87\n",
      "Episode 4900/10000. Epsilon: 0.050. Reward in last 100 episodes: 81.05\n",
      "Episode 4950/10000. Epsilon: 0.050. Reward in last 100 episodes: 81.54\n",
      "Episode 5000/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.56\n",
      "Episode 5050/10000. Epsilon: 0.050. Reward in last 100 episodes: 87.94\n",
      "Episode 5100/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.79\n",
      "Episode 5150/10000. Epsilon: 0.050. Reward in last 100 episodes: 80.43\n",
      "Episode 5200/10000. Epsilon: 0.050. Reward in last 100 episodes: 83.26\n",
      "Episode 5250/10000. Epsilon: 0.050. Reward in last 100 episodes: 89.81\n",
      "Episode 5300/10000. Epsilon: 0.050. Reward in last 100 episodes: 82.14\n",
      "Episode 5350/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.97\n",
      "Episode 5400/10000. Epsilon: 0.050. Reward in last 100 episodes: 77.94\n",
      "Episode 5450/10000. Epsilon: 0.050. Reward in last 100 episodes: 84.58\n",
      "Episode 5500/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.94\n",
      "Episode 5550/10000. Epsilon: 0.050. Reward in last 100 episodes: 84.41\n",
      "Episode 5600/10000. Epsilon: 0.050. Reward in last 100 episodes: 82.51\n",
      "Episode 5650/10000. Epsilon: 0.050. Reward in last 100 episodes: 83.51\n",
      "Episode 5700/10000. Epsilon: 0.050. Reward in last 100 episodes: 90.46\n",
      "Episode 5750/10000. Epsilon: 0.050. Reward in last 100 episodes: 90.84\n",
      "Episode 5800/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.15\n",
      "Episode 5850/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.08\n",
      "Episode 5900/10000. Epsilon: 0.050. Reward in last 100 episodes: 87.27\n",
      "Episode 5950/10000. Epsilon: 0.050. Reward in last 100 episodes: 93.36\n",
      "Episode 6000/10000. Epsilon: 0.050. Reward in last 100 episodes: 94.62\n",
      "Episode 6050/10000. Epsilon: 0.050. Reward in last 100 episodes: 86.39\n",
      "Episode 6100/10000. Epsilon: 0.050. Reward in last 100 episodes: 82.29\n",
      "Episode 6150/10000. Epsilon: 0.050. Reward in last 100 episodes: 86.60\n",
      "Episode 6200/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.36\n",
      "Episode 6250/10000. Epsilon: 0.050. Reward in last 100 episodes: 81.96\n",
      "Episode 6300/10000. Epsilon: 0.050. Reward in last 100 episodes: 92.11\n",
      "Episode 6350/10000. Epsilon: 0.050. Reward in last 100 episodes: 92.40\n",
      "Episode 6400/10000. Epsilon: 0.050. Reward in last 100 episodes: 76.41\n",
      "Episode 6450/10000. Epsilon: 0.050. Reward in last 100 episodes: 71.67\n",
      "Episode 6500/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.11\n",
      "Episode 6550/10000. Epsilon: 0.050. Reward in last 100 episodes: 98.73\n",
      "Episode 6600/10000. Epsilon: 0.050. Reward in last 100 episodes: 96.94\n",
      "Episode 6650/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.55\n",
      "Episode 6700/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.19\n",
      "Episode 6750/10000. Epsilon: 0.050. Reward in last 100 episodes: 92.84\n",
      "Episode 6800/10000. Epsilon: 0.050. Reward in last 100 episodes: 91.88\n",
      "Episode 6850/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.52\n",
      "Episode 6900/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.50\n",
      "Episode 6950/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.80\n",
      "Episode 7000/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.50\n",
      "Episode 7050/10000. Epsilon: 0.050. Reward in last 100 episodes: 95.66\n",
      "Episode 7100/10000. Epsilon: 0.050. Reward in last 100 episodes: 100.30\n",
      "Episode 7150/10000. Epsilon: 0.050. Reward in last 100 episodes: 100.58\n",
      "Episode 7200/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.03\n",
      "Episode 7250/10000. Epsilon: 0.050. Reward in last 100 episodes: 88.54\n",
      "Episode 7300/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.34\n",
      "Episode 7350/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.54\n",
      "Episode 7400/10000. Epsilon: 0.050. Reward in last 100 episodes: 89.49\n",
      "Episode 7450/10000. Epsilon: 0.050. Reward in last 100 episodes: 90.59\n",
      "Episode 7500/10000. Epsilon: 0.050. Reward in last 100 episodes: 93.08\n",
      "Episode 7550/10000. Epsilon: 0.050. Reward in last 100 episodes: 91.61\n",
      "Episode 7600/10000. Epsilon: 0.050. Reward in last 100 episodes: 92.09\n",
      "Episode 7650/10000. Epsilon: 0.050. Reward in last 100 episodes: 94.96\n",
      "Episode 7700/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.83\n",
      "Episode 7750/10000. Epsilon: 0.050. Reward in last 100 episodes: 102.42\n",
      "Episode 7800/10000. Epsilon: 0.050. Reward in last 100 episodes: 104.79\n",
      "Episode 7850/10000. Epsilon: 0.050. Reward in last 100 episodes: 102.25\n",
      "Episode 7900/10000. Epsilon: 0.050. Reward in last 100 episodes: 99.94\n",
      "Episode 7950/10000. Epsilon: 0.050. Reward in last 100 episodes: 96.85\n",
      "Episode 8000/10000. Epsilon: 0.050. Reward in last 100 episodes: 87.91\n",
      "Episode 8050/10000. Epsilon: 0.050. Reward in last 100 episodes: 91.59\n",
      "Episode 8100/10000. Epsilon: 0.050. Reward in last 100 episodes: 98.39\n",
      "Episode 8150/10000. Epsilon: 0.050. Reward in last 100 episodes: 94.18\n",
      "Episode 8200/10000. Epsilon: 0.050. Reward in last 100 episodes: 90.07\n",
      "Episode 8250/10000. Epsilon: 0.050. Reward in last 100 episodes: 90.22\n",
      "Episode 8300/10000. Epsilon: 0.050. Reward in last 100 episodes: 89.72\n",
      "Episode 8350/10000. Epsilon: 0.050. Reward in last 100 episodes: 93.08\n",
      "Episode 8400/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.88\n",
      "Episode 8450/10000. Epsilon: 0.050. Reward in last 100 episodes: 99.38\n",
      "Episode 8500/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.45\n",
      "Episode 8550/10000. Epsilon: 0.050. Reward in last 100 episodes: 101.56\n",
      "Episode 8600/10000. Epsilon: 0.050. Reward in last 100 episodes: 98.85\n",
      "Episode 8650/10000. Epsilon: 0.050. Reward in last 100 episodes: 93.96\n",
      "Episode 8700/10000. Epsilon: 0.050. Reward in last 100 episodes: 85.99\n",
      "Episode 8750/10000. Epsilon: 0.050. Reward in last 100 episodes: 87.36\n",
      "Episode 8800/10000. Epsilon: 0.050. Reward in last 100 episodes: 92.32\n",
      "Episode 8850/10000. Epsilon: 0.050. Reward in last 100 episodes: 96.29\n",
      "Episode 8900/10000. Epsilon: 0.050. Reward in last 100 episodes: 110.66\n",
      "Episode 8950/10000. Epsilon: 0.050. Reward in last 100 episodes: 110.75\n",
      "Episode 9000/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.20\n",
      "Episode 9050/10000. Epsilon: 0.050. Reward in last 100 episodes: 91.73\n",
      "Episode 9100/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.09\n",
      "Episode 9150/10000. Epsilon: 0.050. Reward in last 100 episodes: 99.19\n",
      "Episode 9200/10000. Epsilon: 0.050. Reward in last 100 episodes: 104.12\n",
      "Episode 9250/10000. Epsilon: 0.050. Reward in last 100 episodes: 102.51\n",
      "Episode 9300/10000. Epsilon: 0.050. Reward in last 100 episodes: 93.77\n",
      "Episode 9350/10000. Epsilon: 0.050. Reward in last 100 episodes: 96.97\n",
      "Episode 9400/10000. Epsilon: 0.050. Reward in last 100 episodes: 108.43\n",
      "Episode 9450/10000. Epsilon: 0.050. Reward in last 100 episodes: 99.77\n",
      "Episode 9500/10000. Epsilon: 0.050. Reward in last 100 episodes: 99.01\n",
      "Episode 9550/10000. Epsilon: 0.050. Reward in last 100 episodes: 101.54\n",
      "Episode 9600/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.58\n",
      "Episode 9650/10000. Epsilon: 0.050. Reward in last 100 episodes: 104.21\n",
      "Episode 9700/10000. Epsilon: 0.050. Reward in last 100 episodes: 96.74\n",
      "Episode 9750/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.53\n",
      "Episode 9800/10000. Epsilon: 0.050. Reward in last 100 episodes: 109.35\n",
      "Episode 9850/10000. Epsilon: 0.050. Reward in last 100 episodes: 102.44\n",
      "Episode 9900/10000. Epsilon: 0.050. Reward in last 100 episodes: 97.39\n",
      "Episode 9950/10000. Epsilon: 0.050. Reward in last 100 episodes: 89.33\n",
      "Episode 10000/10000. Epsilon: 0.050. Reward in last 100 episodes: 94.61\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 10000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayMemory(100000, device=device)\n",
    "cur_frame = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "  state = env.reset()[0].astype(np.float32)\n",
    "  ep_reward, done = 0, False\n",
    "  while not done:\n",
    "    state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "    next_state, reward, done, info, unknown_attribute = env.step(action)\n",
    "    next_state = next_state.astype(np.float32)\n",
    "    ep_reward += reward\n",
    "    # Save to experience replay.\n",
    "    integer_action = (action + 1)*(ACTIONS/2)\n",
    "    #print(f\"the actions is {action} and its integer {integer_action} multiplied by {ACTIONS}\")\n",
    "    buffer.add(state, integer_action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    cur_frame += 1\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 2000 == 0:\n",
    "      target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "    # Train neural network.\n",
    "    if len(buffer) > batch_size:\n",
    "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "      next_states = torch.from_numpy(next_states)\n",
    "      rewards = torch.from_numpy(rewards)\n",
    "      states = torch.from_numpy(states)\n",
    "      dones = torch.from_numpy(dones)\n",
    "      actions = torch.from_numpy(actions)\n",
    "\n",
    "\n",
    "      loss = training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "  if episode < 950:\n",
    "    epsilon -= 0.001\n",
    "\n",
    "  if len(last_100_ep_rewards) == 100:\n",
    "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "  last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "  if episode % 50 == 0:\n",
    "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNon è possibile eseguire il codice. La sessione è stata eliminata. Provare a riavviare il kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mNon è possibile eseguire il codice. La sessione è stata eliminata. Provare a riavviare il kernel."
     ]
    }
   ],
   "source": [
    "\n",
    "env_test = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\")\n",
    "\n",
    "observation = env_test.reset()[0].astype(np.float32)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    inp = torch.from_numpy(observation)\n",
    "    action = select_epsilon_greedy_action(inp,0)\n",
    "    observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "    observation = observation.astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "\n",
    "\n",
    "env_test.close"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"./\"\n",
    "torch.save(main_nn.state_dict(), PATH + r\"RNN.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to import a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"./\"\n",
    "main_nn.load_state_dict(torch.load(PATH + r\"RNN5-layers2.pth\",map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
