{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Machine Learning\n",
    "#### Double inverted pendulum - Lorenzo Frangella 1899674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run in only in colab for packet download\n",
    "!pip3 install gymnasium\n",
    "!pip3 install gym[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-26 13:27:27.272549: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math \n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters used in our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 1e-8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/glfw/__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# test of the environment\n",
    "\n",
    "env = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\") #change w \"human\" if needed\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close\n",
    "\n",
    "\n",
    "OBSERVATIONS = env.observation_space.shape[0]\n",
    "ACTIONS = 64\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the output of the neural network to the interval -1,1\n",
    "Since as action we have only one scalar value that represent magnitude and direction of the force applied to the cart, we have to \"cast\" the output of the neural network into a scalar contiguos value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_continue_action(index):\n",
    "    value = (2/ACTIONS) * index - 1\n",
    "    return value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the neural network needed for DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 128)\n",
    "        self.layer5 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "print(OBSERVATIONS)\n",
    "\n",
    "main_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "target_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data structure that is needed for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, size, device =\"cpu\"):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self,state,action, reward, next_state, done):\n",
    "        self.buffer.append((state, action,reward,next_state,done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [] , [] , [] , [] , []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part of code is needed to develop a epsilon-greedy policy\n",
    "\n",
    "With a given probability epsilon a random action is choosen otherwise is choosen the best action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        action_decimal = np.array([(np.argmax(qs)*(2/ACTIONS))-1])\n",
    "        return action_decimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a train step that is performed on the replay memory of size batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(states, actions, rewards, next_states, dones):\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * DISCOUNT * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions.type(torch.int64), ACTIONS)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000. Epsilon: 0.999. Reward in last 100 episodes: 64.08, loss: 0\n",
      "Episode 50/10000. Epsilon: 0.949. Reward in last 100 episodes: 61.62, loss: 9624.318359375\n",
      "Episode 100/10000. Epsilon: 0.899. Reward in last 100 episodes: 62.33, loss: 9578.0849609375\n",
      "Episode 150/10000. Epsilon: 0.849. Reward in last 100 episodes: 65.46, loss: 13421.8642578125\n",
      "Episode 200/10000. Epsilon: 0.799. Reward in last 100 episodes: 65.38, loss: 14449.2890625\n",
      "Episode 250/10000. Epsilon: 0.749. Reward in last 100 episodes: 67.25, loss: 9710.49609375\n",
      "Episode 300/10000. Epsilon: 0.699. Reward in last 100 episodes: 71.12, loss: 881.7188110351562\n",
      "Episode 350/10000. Epsilon: 0.649. Reward in last 100 episodes: 73.01, loss: 251.05413818359375\n",
      "Episode 400/10000. Epsilon: 0.599. Reward in last 100 episodes: 79.08, loss: 1554.2393798828125\n",
      "Episode 450/10000. Epsilon: 0.549. Reward in last 100 episodes: 88.06, loss: 928.8261108398438\n",
      "Episode 500/10000. Epsilon: 0.499. Reward in last 100 episodes: 92.67, loss: 2449.876220703125\n",
      "Episode 550/10000. Epsilon: 0.449. Reward in last 100 episodes: 90.81, loss: 209.62779235839844\n",
      "Episode 600/10000. Epsilon: 0.399. Reward in last 100 episodes: 99.13, loss: 664.2157592773438\n",
      "Episode 650/10000. Epsilon: 0.349. Reward in last 100 episodes: 116.19, loss: 58.13908767700195\n",
      "Episode 700/10000. Epsilon: 0.299. Reward in last 100 episodes: 122.91, loss: 1457.178955078125\n",
      "Episode 750/10000. Epsilon: 0.249. Reward in last 100 episodes: 135.82, loss: 885.452880859375\n",
      "Episode 800/10000. Epsilon: 0.199. Reward in last 100 episodes: 176.45, loss: 80.88296508789062\n",
      "Episode 850/10000. Epsilon: 0.149. Reward in last 100 episodes: 212.24, loss: 64.08190155029297\n",
      "Episode 900/10000. Epsilon: 0.099. Reward in last 100 episodes: 289.31, loss: 298.155029296875\n",
      "Episode 950/10000. Epsilon: 0.050. Reward in last 100 episodes: 427.19, loss: 977.2537841796875\n",
      "Episode 1000/10000. Epsilon: 0.050. Reward in last 100 episodes: 552.78, loss: 541.3353881835938\n",
      "Episode 1050/10000. Epsilon: 0.050. Reward in last 100 episodes: 637.07, loss: 1219.317626953125\n",
      "Episode 1100/10000. Epsilon: 0.050. Reward in last 100 episodes: 605.91, loss: 837.2453002929688\n",
      "Episode 1150/10000. Epsilon: 0.050. Reward in last 100 episodes: 572.96, loss: 159.0288543701172\n",
      "Episode 1200/10000. Epsilon: 0.050. Reward in last 100 episodes: 686.50, loss: 398.474609375\n",
      "Episode 1250/10000. Epsilon: 0.050. Reward in last 100 episodes: 727.71, loss: 532.7570190429688\n",
      "Episode 1300/10000. Epsilon: 0.050. Reward in last 100 episodes: 683.08, loss: 30.77886390686035\n",
      "Episode 1350/10000. Epsilon: 0.050. Reward in last 100 episodes: 658.29, loss: 1725.242431640625\n",
      "Episode 1400/10000. Epsilon: 0.050. Reward in last 100 episodes: 608.64, loss: 230.47610473632812\n",
      "Episode 1450/10000. Epsilon: 0.050. Reward in last 100 episodes: 627.27, loss: 857.6070556640625\n",
      "Episode 1500/10000. Epsilon: 0.050. Reward in last 100 episodes: 613.86, loss: 271.30511474609375\n",
      "Episode 1550/10000. Epsilon: 0.050. Reward in last 100 episodes: 556.73, loss: 179.78964233398438\n",
      "Episode 1600/10000. Epsilon: 0.050. Reward in last 100 episodes: 542.07, loss: 346.9749450683594\n",
      "Episode 1650/10000. Epsilon: 0.050. Reward in last 100 episodes: 498.02, loss: 500.9063720703125\n",
      "Episode 1700/10000. Epsilon: 0.050. Reward in last 100 episodes: 565.08, loss: 187.49427795410156\n",
      "Episode 1750/10000. Epsilon: 0.050. Reward in last 100 episodes: 753.65, loss: 28.734827041625977\n",
      "Episode 1800/10000. Epsilon: 0.050. Reward in last 100 episodes: 735.20, loss: 520.134521484375\n",
      "Episode 1850/10000. Epsilon: 0.050. Reward in last 100 episodes: 601.01, loss: 58.59774398803711\n",
      "Episode 1900/10000. Epsilon: 0.050. Reward in last 100 episodes: 593.45, loss: 547.325439453125\n",
      "Episode 1950/10000. Epsilon: 0.050. Reward in last 100 episodes: 587.59, loss: 280.15673828125\n",
      "Episode 2000/10000. Epsilon: 0.050. Reward in last 100 episodes: 595.04, loss: 25.283687591552734\n",
      "Episode 2050/10000. Epsilon: 0.050. Reward in last 100 episodes: 586.73, loss: 387.72515869140625\n",
      "Episode 2100/10000. Epsilon: 0.050. Reward in last 100 episodes: 532.18, loss: 39.10700607299805\n",
      "Episode 2150/10000. Epsilon: 0.050. Reward in last 100 episodes: 557.50, loss: 275.1522216796875\n",
      "Episode 2200/10000. Epsilon: 0.050. Reward in last 100 episodes: 626.44, loss: 167.47372436523438\n",
      "Episode 2250/10000. Epsilon: 0.050. Reward in last 100 episodes: 659.08, loss: 418.365234375\n",
      "Episode 2300/10000. Epsilon: 0.050. Reward in last 100 episodes: 704.34, loss: 126.94762420654297\n",
      "Episode 2350/10000. Epsilon: 0.050. Reward in last 100 episodes: 679.42, loss: 8.534502029418945\n",
      "Episode 2400/10000. Epsilon: 0.050. Reward in last 100 episodes: 614.82, loss: 91.77693939208984\n",
      "Episode 2450/10000. Epsilon: 0.050. Reward in last 100 episodes: 678.26, loss: 95.46560668945312\n",
      "Episode 2500/10000. Epsilon: 0.050. Reward in last 100 episodes: 746.83, loss: 40.859703063964844\n",
      "Episode 2550/10000. Epsilon: 0.050. Reward in last 100 episodes: 619.15, loss: 56.86698532104492\n",
      "Episode 2600/10000. Epsilon: 0.050. Reward in last 100 episodes: 557.77, loss: 40.3839111328125\n",
      "Episode 2650/10000. Epsilon: 0.050. Reward in last 100 episodes: 600.52, loss: 12.174647331237793\n",
      "Episode 2700/10000. Epsilon: 0.050. Reward in last 100 episodes: 638.58, loss: 8.549041748046875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb Cella 18\u001b[0m in \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     dones \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(dones)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     actions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(actions)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     loss \u001b[39m=\u001b[39m training_step(states, actions, rewards, next_states, dones)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m episode \u001b[39m<\u001b[39m \u001b[39m950\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m   epsilon \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n",
      "\u001b[1;32m/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb Cella 18\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(masked_qs, target\u001b[39m.\u001b[39mdetach())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 10000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayMemory(100000, device=device)\n",
    "cur_frame = 0\n",
    "loss = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "  state = env.reset()[0].astype(np.float32)\n",
    "  ep_reward, done = 0, False\n",
    "  while not done:\n",
    "    state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "    next_state, reward, done, info, unknown_attribute = env.step(action)\n",
    "    next_state = next_state.astype(np.float32)\n",
    "    ep_reward += reward\n",
    "    # Save to experience replay.\n",
    "    integer_action = (action + 1)*(ACTIONS/2)\n",
    "    #print(f\"the actions is {action} and its integer {integer_action} multiplied by {ACTIONS}\")\n",
    "    buffer.add(state, integer_action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    cur_frame += 1\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 2000 == 0:\n",
    "      target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "    # Train neural network.\n",
    "    if len(buffer) > batch_size:\n",
    "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "      next_states = torch.from_numpy(next_states)\n",
    "      rewards = torch.from_numpy(rewards)\n",
    "      states = torch.from_numpy(states)\n",
    "      dones = torch.from_numpy(dones)\n",
    "      actions = torch.from_numpy(actions)\n",
    "\n",
    "\n",
    "      loss = training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "  if episode < 950:\n",
    "    epsilon -= 0.001\n",
    "\n",
    "  if len(last_100_ep_rewards) == 100:\n",
    "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "  last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "  if episode % 50 == 0:\n",
    "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}, loss: {loss}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_test = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\")\n",
    "\n",
    "observation = env_test.reset()[0].astype(np.float32)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    inp = torch.from_numpy(observation)\n",
    "    action = select_epsilon_greedy_action(inp,0)\n",
    "    observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "    observation = observation.astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "\n",
    "\n",
    "env_test.close"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"./\"\n",
    "torch.save(main_nn.state_dict(), PATH + r\"RNN.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to import a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"./\"\n",
    "main_nn.load_state_dict(torch.load(PATH + r\"RNN5-layers2.pth\",map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
