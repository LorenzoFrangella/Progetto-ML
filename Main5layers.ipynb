{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Machine Learning\n",
    "#### Double inverted pendulum - Lorenzo Frangella 1899674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run in only in colab for packet download\n",
    "!pip3 install gymnasium\n",
    "!pip3 install gym[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math \n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters used in our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 1e-4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/glfw/__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# test of the environment\n",
    "\n",
    "env = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\") #change w \"human\" if needed\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close\n",
    "\n",
    "\n",
    "OBSERVATIONS = env.observation_space.shape[0]\n",
    "ACTIONS = 64\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the output of the neural network to the interval -1,1\n",
    "Since as action we have only one scalar value that represent magnitude and direction of the force applied to the cart, we have to \"cast\" the output of the neural network into a scalar contiguos value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_continue_action(index):\n",
    "    value = (2/ACTIONS) * index - 1\n",
    "    return value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the neural network needed for DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 128)\n",
    "        self.layer5 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "print(OBSERVATIONS)\n",
    "\n",
    "main_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "target_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data structure that is needed for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, size, device =\"cpu\"):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self,state,action, reward, next_state, done):\n",
    "        self.buffer.append((state, action,reward,next_state,done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [] , [] , [] , [] , []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part of code is needed to develop a epsilon-greedy policy\n",
    "\n",
    "With a given probability epsilon a random action is choosen otherwise is choosen the best action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        action_decimal = np.array([(np.argmax(qs)*(2/ACTIONS))-1])\n",
    "        return action_decimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a train step that is performed on the replay memory of size batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(states, actions, rewards, next_states, dones):\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * DISCOUNT * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions.type(torch.int64), ACTIONS)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 10000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayMemory(100000, device=device)\n",
    "cur_frame = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "  state = env.reset()[0].astype(np.float32)\n",
    "  ep_reward, done = 0, False\n",
    "  while not done:\n",
    "    state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "    next_state, reward, done, info, unknown_attribute = env.step(action)\n",
    "    next_state = next_state.astype(np.float32)\n",
    "    ep_reward += reward\n",
    "    # Save to experience replay.\n",
    "    integer_action = (action + 1)*(ACTIONS/2)\n",
    "    #print(f\"the actions is {action} and its integer {integer_action} multiplied by {ACTIONS}\")\n",
    "    buffer.add(state, integer_action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    cur_frame += 1\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 2000 == 0:\n",
    "      target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "    # Train neural network.\n",
    "    if len(buffer) > batch_size:\n",
    "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "      next_states = torch.from_numpy(next_states)\n",
    "      rewards = torch.from_numpy(rewards)\n",
    "      states = torch.from_numpy(states)\n",
    "      dones = torch.from_numpy(dones)\n",
    "      actions = torch.from_numpy(actions)\n",
    "\n",
    "\n",
    "      loss = training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "  if episode < 950:\n",
    "    epsilon -= 0.001\n",
    "\n",
    "  if len(last_100_ep_rewards) == 100:\n",
    "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "  last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "  if episode % 50 == 0:\n",
    "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Wrapper.close of <TimeLimit<OrderEnforcing<PassiveEnvChecker<InvertedDoublePendulumEnv<InvertedDoublePendulum-v4>>>>>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env_test = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\")\n",
    "\n",
    "observation = env_test.reset()[0].astype(np.float32)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    inp = torch.from_numpy(observation)\n",
    "    action = select_epsilon_greedy_action(inp,0)\n",
    "    observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "    observation = observation.astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "\n",
    "\n",
    "env_test.close"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"./\"\n",
    "torch.save(main_nn.state_dict(), PATH + r\"RNN.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to import a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"./\"\n",
    "main_nn.load_state_dict(torch.load(PATH + r\"RNN5-layers2.pth\",map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
