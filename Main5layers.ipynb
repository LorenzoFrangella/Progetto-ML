{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progetto di Machine Learning\n",
    "#### Double inverted pendulum - Lorenzo Frangella 1899674"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to run in only in colab for packet download\n",
    "!pip3 install gymnasium\n",
    "!pip3 install gym[mujoco]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-27 15:36:30.891682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math \n",
    "import os \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters used in our neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.95\n",
    "LEARNING_RATE = 1e-8\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/glfw/__init__.py:916: GLFWError: (65537) b'The GLFW library is not initialized'\n",
      "  warnings.warn(message, GLFWError)\n"
     ]
    }
   ],
   "source": [
    "# test of the environment\n",
    "\n",
    "env = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\") #change w \"human\" if needed\n",
    "observation, info = env.reset()\n",
    "\n",
    "for i in range(1000):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close\n",
    "\n",
    "\n",
    "OBSERVATIONS = env.observation_space.shape[0]\n",
    "ACTIONS = 64\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the output of the neural network to the interval -1,1\n",
    "Since as action we have only one scalar value that represent magnitude and direction of the force applied to the cart, we have to \"cast\" the output of the neural network into a scalar contiguos value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_to_continue_action(index):\n",
    "    value = (2/ACTIONS) * index - 1\n",
    "    return value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the structure of the neural network needed for DQN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer4 = nn.Linear(128, 128)\n",
    "        self.layer5 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = F.relu(self.layer4(x))\n",
    "        x = F.relu(self.layer5(x))\n",
    "        return self.layer3(x)\n",
    "    \n",
    "print(OBSERVATIONS)\n",
    "\n",
    "main_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "target_nn = DQN(OBSERVATIONS,ACTIONS).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(main_nn.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the data structure that is needed for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, size, device =\"cpu\"):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self,state,action, reward, next_state, done):\n",
    "        self.buffer.append((state, action,reward,next_state,done))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def sample(self, num_samples):\n",
    "        states, actions, rewards, next_states, dones = [] , [] , [] , [] , []\n",
    "        idx = np.random.choice(len(self.buffer), num_samples)\n",
    "        for i in idx:\n",
    "            elem = self.buffer[i]\n",
    "            state, action, reward, next_state, done = elem\n",
    "            states.append(np.array(state, copy=False))\n",
    "            actions.append(np.array(action, copy=False))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(np.array(next_state, copy=False))\n",
    "            dones.append(done)\n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards, dtype=np.float32)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones, dtype=np.float32)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "                    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following part of code is needed to develop a epsilon-greedy policy\n",
    "\n",
    "With a given probability epsilon a random action is choosen otherwise is choosen the best action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_epsilon_greedy_action(state, epsilon):\n",
    "    result = np.random.uniform()\n",
    "    if result < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        qs = main_nn(state).cpu().data.numpy()\n",
    "        action_decimal = np.array([(np.argmax(qs)*(2/ACTIONS))-1])\n",
    "        return action_decimal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of a train step that is performed on the replay memory of size batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(states, actions, rewards, next_states, dones):\n",
    "    max_next_qs = target_nn(next_states).max(-1).values\n",
    "    target = rewards + (1.0 - dones) * DISCOUNT * max_next_qs\n",
    "    qs = main_nn(states)\n",
    "    action_masks = F.one_hot(actions.type(torch.int64), ACTIONS)\n",
    "    masked_qs = (action_masks * qs).sum(dim=-1)\n",
    "    loss = loss_fn(masked_qs, target.detach())\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/10000. Epsilon: 0.999. Reward in last 100 episodes: 45.02, loss: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 32])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/10000. Epsilon: 0.949. Reward in last 100 episodes: 56.27, loss: 84.03321838378906\n",
      "Episode 100/10000. Epsilon: 0.899. Reward in last 100 episodes: 54.77, loss: 84.7771987915039\n",
      "Episode 150/10000. Epsilon: 0.849. Reward in last 100 episodes: 56.37, loss: 83.63048553466797\n",
      "Episode 200/10000. Epsilon: 0.799. Reward in last 100 episodes: 58.34, loss: 83.06416320800781\n",
      "Episode 250/10000. Epsilon: 0.749. Reward in last 100 episodes: 55.91, loss: 84.69487762451172\n",
      "Episode 300/10000. Epsilon: 0.699. Reward in last 100 episodes: 54.21, loss: 84.53184509277344\n",
      "Episode 350/10000. Epsilon: 0.649. Reward in last 100 episodes: 52.95, loss: 84.54840087890625\n",
      "Episode 400/10000. Epsilon: 0.599. Reward in last 100 episodes: 51.72, loss: 83.67645263671875\n",
      "Episode 450/10000. Epsilon: 0.549. Reward in last 100 episodes: 52.61, loss: 84.02906799316406\n",
      "Episode 500/10000. Epsilon: 0.499. Reward in last 100 episodes: 51.59, loss: 83.78569793701172\n",
      "Episode 550/10000. Epsilon: 0.449. Reward in last 100 episodes: 50.75, loss: 85.57537078857422\n",
      "Episode 600/10000. Epsilon: 0.399. Reward in last 100 episodes: 52.79, loss: 84.20049285888672\n",
      "Episode 650/10000. Epsilon: 0.349. Reward in last 100 episodes: 51.02, loss: 84.94947814941406\n",
      "Episode 700/10000. Epsilon: 0.299. Reward in last 100 episodes: 50.19, loss: 82.9313735961914\n",
      "Episode 750/10000. Epsilon: 0.249. Reward in last 100 episodes: 47.65, loss: 82.41897583007812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb Cella 18\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m state_in \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mexpand_dims(state, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m action \u001b[39m=\u001b[39m select_epsilon_greedy_action(state_in, epsilon)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m next_state, reward, done, info, unknown_attribute \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m next_state \u001b[39m=\u001b[39m next_state\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lorenzo/Documents/GitHub/Progetto-ML/Main5layers.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m ep_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m     \u001b[39m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gymnasium/wrappers/env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gymnasium/envs/mujoco/inverted_double_pendulum_v4.py:150\u001b[0m, in \u001b[0;36mInvertedDoublePendulumEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_simulation(action, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mframe_skip)\n\u001b[0;32m--> 150\u001b[0m     ob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_obs()\n\u001b[1;32m    151\u001b[0m     x, _, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39msite_xpos[\u001b[39m0\u001b[39m]\n\u001b[1;32m    152\u001b[0m     dist_penalty \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m \u001b[39m*\u001b[39m x\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m (y \u001b[39m-\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/gymnasium/envs/mujoco/inverted_double_pendulum_v4.py:169\u001b[0m, in \u001b[0;36mInvertedDoublePendulumEnv._get_obs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_obs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    163\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mconcatenate(\n\u001b[1;32m    164\u001b[0m         [\n\u001b[1;32m    165\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mqpos[:\u001b[39m1\u001b[39m],  \u001b[39m# cart x pos\u001b[39;00m\n\u001b[1;32m    166\u001b[0m             np\u001b[39m.\u001b[39msin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mqpos[\u001b[39m1\u001b[39m:]),  \u001b[39m# link angles\u001b[39;00m\n\u001b[1;32m    167\u001b[0m             np\u001b[39m.\u001b[39mcos(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mqpos[\u001b[39m1\u001b[39m:]),\n\u001b[1;32m    168\u001b[0m             np\u001b[39m.\u001b[39mclip(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mqvel, \u001b[39m-\u001b[39m\u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m),\n\u001b[0;32m--> 169\u001b[0m             np\u001b[39m.\u001b[39;49mclip(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mqfrc_constraint, \u001b[39m-\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m10\u001b[39;49m),\n\u001b[1;32m    170\u001b[0m         ]\n\u001b[1;32m    171\u001b[0m     )\u001b[39m.\u001b[39mravel()\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:2154\u001b[0m, in \u001b[0;36mclip\u001b[0;34m(a, a_min, a_max, out, **kwargs)\u001b[0m\n\u001b[1;32m   2085\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_clip_dispatcher)\n\u001b[1;32m   2086\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclip\u001b[39m(a, a_min, a_max, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   2087\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m \u001b[39m    Clip (limit) the values in an array.\u001b[39;00m\n\u001b[1;32m   2089\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2152\u001b[0m \n\u001b[1;32m   2153\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mclip\u001b[39;49m\u001b[39m'\u001b[39;49m, a_min, a_max, out\u001b[39m=\u001b[39;49mout, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:57\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[39m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[39m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[39m# exception has a traceback chain.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/_methods.py:135\u001b[0m, in \u001b[0;36m_clip\u001b[0;34m(a, min, max, out, casting, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _clip_dep_is_byte_swapped(a) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _clip_dep_is_byte_swapped(out):\n\u001b[1;32m    134\u001b[0m     using_deprecated_nan \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mif\u001b[39;00m _clip_dep_is_scalar_nan(\u001b[39mmin\u001b[39;49m):\n\u001b[1;32m    136\u001b[0m         \u001b[39mmin\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m         using_deprecated_nan \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/_methods.py:95\u001b[0m, in \u001b[0;36m_clip_dep_is_scalar_nan\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_clip_dep_is_scalar_nan\u001b[39m(a):\n\u001b[1;32m     93\u001b[0m     \u001b[39m# guarded to protect circular imports\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfromnumeric\u001b[39;00m \u001b[39mimport\u001b[39;00m ndim\n\u001b[0;32m---> 95\u001b[0m     \u001b[39mif\u001b[39;00m ndim(a) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     96\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3117\u001b[0m, in \u001b[0;36m_ndim_dispatcher\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3055\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3056\u001b[0m \u001b[39m    Return the cumulative product of elements along a given axis.\u001b[39;00m\n\u001b[1;32m   3057\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3112\u001b[0m \n\u001b[1;32m   3113\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3114\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39m\u001b[39mcumprod\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout)\n\u001b[0;32m-> 3117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ndim_dispatcher\u001b[39m(a):\n\u001b[1;32m   3118\u001b[0m     \u001b[39mreturn\u001b[39;00m (a,)\n\u001b[1;32m   3121\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_ndim_dispatcher)\n\u001b[1;32m   3122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mndim\u001b[39m(a):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mSi è verificato un arresto anomalo del kernel durante l'esecuzione del codice nella cella attiva o in una cella precedente. Esaminare il codice nelle celle per identificare una possibile causa dell'errore. Per altre informazioni, fare clic su <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a>. Per altri dettagli, vedere Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "# Hyperparameters.\n",
    "num_episodes = 10000\n",
    "epsilon = 1.0\n",
    "batch_size = 32\n",
    "discount = 0.99\n",
    "buffer = ReplayMemory(100000, device=device)\n",
    "cur_frame = 0\n",
    "loss = 0\n",
    "\n",
    "# Start training. Play game once and then train with a batch.\n",
    "last_100_ep_rewards = []\n",
    "for episode in range(num_episodes+1):\n",
    "  state = env.reset()[0].astype(np.float32)\n",
    "  ep_reward, done = 0, False\n",
    "  while not done:\n",
    "    state_in = torch.from_numpy(np.expand_dims(state, axis=0)).to(device)\n",
    "    action = select_epsilon_greedy_action(state_in, epsilon)\n",
    "    next_state, reward, done, info, unknown_attribute = env.step(action)\n",
    "    next_state = next_state.astype(np.float32)\n",
    "    ep_reward += reward\n",
    "    # Save to experience replay.\n",
    "    integer_action = (action + 1)*(ACTIONS/2)\n",
    "    #print(f\"the actions is {action} and its integer {integer_action} multiplied by {ACTIONS}\")\n",
    "    buffer.add(state, integer_action, reward, next_state, done)\n",
    "    state = next_state\n",
    "    cur_frame += 1\n",
    "    # Copy main_nn weights to target_nn.\n",
    "    if cur_frame % 2000 == 0:\n",
    "      target_nn.load_state_dict(main_nn.state_dict())\n",
    "    \n",
    "    # Train neural network.\n",
    "    if len(buffer) > batch_size:\n",
    "      states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
    "      next_states = torch.from_numpy(next_states)\n",
    "      rewards = torch.from_numpy(rewards)\n",
    "      states = torch.from_numpy(states)\n",
    "      dones = torch.from_numpy(dones)\n",
    "      actions = torch.from_numpy(actions)\n",
    "\n",
    "\n",
    "      loss = training_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "  if episode < 950:\n",
    "    epsilon -= 0.001\n",
    "\n",
    "  if len(last_100_ep_rewards) == 100:\n",
    "    last_100_ep_rewards = last_100_ep_rewards[1:]\n",
    "  last_100_ep_rewards.append(ep_reward)\n",
    "\n",
    "  if episode % 50 == 0:\n",
    "    print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}.'\n",
    "          f' Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.2f}, loss: {loss}')\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env_test = gym.make('InvertedDoublePendulum-v4',render_mode=\"human\")\n",
    "\n",
    "observation = env_test.reset()[0].astype(np.float32)\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    inp = torch.from_numpy(observation)\n",
    "    action = select_epsilon_greedy_action(inp,0)\n",
    "    observation, reward, terminated, truncated, info = env_test.step(action)\n",
    "    observation = observation.astype(np.float32)\n",
    "\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "        observation = observation.astype(np.float32)\n",
    "\n",
    "\n",
    "env_test.close"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to export the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r\"./\"\n",
    "torch.save(main_nn.state_dict(), PATH + r\"RNN.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code to import a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = r\"./\"\n",
    "main_nn.load_state_dict(torch.load(PATH + r\"RNN5-layers2.pth\",map_location=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
